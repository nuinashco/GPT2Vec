task: mlm # mlm | mntp
mask_token: "<mask>"


training_args:
    learning_rate: 2e-4
    weight_decay: 0.01
    lr_scheduler_type: cosine
    warmup_ratio: 0.0
    num_train_epochs: 1
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 8
    do_train: True
    do_eval: False
    bf16: False
    report_to: wandb
    optim: adamw_8bit
    save_strategy: steps
    logging_steps: 20
    save_steps: 200