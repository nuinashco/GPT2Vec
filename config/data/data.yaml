dataset:
  path: wikimedia/wikipedia
  name: 20231101.uk
  split: train

sample:
  num_samples: 1000
  seed: ${seed}

tokenize:
  max_length: 512
  truncation: True