training_args:
    learning_rate: 1e-4
    weight_decay: 0.01
    lr_scheduler_type: cosine
    warmup_ratio: 0.0
    num_train_epochs: 5
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 1
    gradient_accumulation_steps: 2
    report_to: wandb
    optim: adamw_8bit
    eval_strategy: steps
    save_strategy: steps
    eval_steps: 200
    logging_steps: 20
    save_steps: 200
    save_total_limit: 10
    metric_for_best_model: eval_f1
    greater_is_better: True
    load_best_model_at_end: True

metric: seqeval