task: mntp # mlm | mntp

# We should use some existing token
mask_token: "_"


training_args:
    learning_rate: 2e-4
    weight_decay: 0.01
    lr_scheduler_type: cosine
    warmup_ratio: 0.0
    max_steps: 2000
    per_device_train_batch_size: 16
    gradient_accumulation_steps: 2
    do_train: True
    do_eval: False
    bf16: True
    report_to: wandb
    optim: adamw_8bit
    save_strategy: steps
    logging_steps: 20
    save_steps: 200