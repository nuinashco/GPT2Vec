model_family: gemma3


model:
  pretrained_model_name_or_path: google/gemma-3-4b-pt
  torch_dtype: bfloat16


# quantization:
#   load_in_4bit: True
#   bnb_4bit_quant_type: nf4
#   bnb_4bit_use_double_quant: False
#   bnb_4bit_compute_dtype: float16


lora:
  r: 64  # the dimension of the low-rank matrices
  lora_alpha: 128 # scaling factor for LoRA activations vs pre-trained weight activations
  lora_dropout: 0.05 
  bias: none
  inference_mode: False
  task_type: CAUSAL_LM
  target_modules: ['o_proj', 'v_proj', "q_proj", "k_proj", "gate_proj", "down_proj", "up_proj"]